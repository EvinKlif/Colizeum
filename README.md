```markdown
# Предварительная установка

1. Перейти в папку `COLIZEUM`.
2. Выполнить команду:

   
   pip install -r requirements.txt
   ```

3. Выполнить команду:

   ```bash
   playwright install
   ```

# Task 1

1. Перейти в папку `Task 1`.
2. Открыть файл `config.py` и вставить ваш `openweather_api_key` и `news_api_key` либо воспользоваться моим, который там есть.
3. Запустить скрипт:

   ```bash
   python .\script.py
   ```

   В этой папке появятся 3 файла:
   - `news_api.xlsx`
   - `users_api.xlsx`
   - `weather_api.xlsx`

# Task 2

1. Перейти в папку `Task 2`.
2. Открыть файл `config.py` и вставить название вашей таблицы в Google Spreadsheets.
3. Положить в папку `Task 2` ваш файл `credentials.json`. Если у вас другое название файла, переименуйте его в `credentials.json`.
4. Запустить скрипт:

   ```bash
   python .\scraper.py
   ```

   В вашей таблице в Google Spreadsheets появится перечень процессоров, отсортированный по цене.

# Task 3: Ответ на 3 вопрос

## Подготовка к миграции:
- **Анализ нагрузки**: выберите время с минимальной нагрузкой на базу данных (например, ночью или в выходные).
- **Резервное копирование**: создайте полную резервную копию базы данных перед внесением изменений.
- **Тестирование**: проверьте миграцию на staging-окружении, чтобы оценить время выполнения и влияние на производительность.

## Создание миграции:
- Добавьте поле в модель Django.

## Оптимизация миграции:
- Добавьте поле без дефолтного значения, чтобы избежать блокировок.
- Заполните поле значением в фоновом режиме.
- Установите дефолтное значение на уровне базы данных.

## Постепенное заполнение данных:
- Используйте фоновые задачи (например, Celery) для обновления данных пачками.
- Мониторьте нагрузку на базу данных и приложение во время выполнения.
- Убедитесь, что все данные корректно обновлены.

## Завершение миграции:
- Удалите временные настройки.
- Создайте новую резервную копию базы данных после успешного завершения.
- Если что-то пошло не так, откатите базу.

# Task 4: Ответ на 4 вопрос

## Проверка блокировок (Locks):
- Если запрос ожидает снятия блокировки, найдите процесс, который удерживает блокировку, и при необходимости завершите его.

## Анализ длительных транзакций:
- Если найдена долгая транзакция, завершите ее или дождитесь завершения.

## Проверка нагрузки на базу данных:
- Анализ логов PostgreSQL.
- Использование `EXPLAIN` для анализа запроса.

# Task 5: Ответ на 5 вопрос

## Создание Dockerfile:
1. Сборка Docker-образа и загрузка в Docker Registry:

   ```bash
   docker build -t your-dockerhub-username/flask-app:latest .
   docker push your-dockerhub-username/flask-app:latest
   ```

2. Создание Kubernetes-манифестов:
   - `deployment.yaml`
   - `service.yaml`
   - `ingress.yaml`

3. Деплой приложения в Kubernetes:

   ```bash
   kubectl apply -f k8s/deployment.yaml
   kubectl apply -f k8s/service.yaml
   kubectl apply -f k8s/ingress.yaml
   ```

4. Проверка статуса деплоя:

   ```bash
   kubectl get pods
   kubectl get services
   kubectl get ingress
   ```

# Task 6: Ответ на 6 вопрос

## Изучите логи access.log:
- Чтобы определить, увеличилось ли время обработки запросов.

## Проверка сетевой задержки и пропускной способности:
- Используйте `ping` и `traceroute`.

## Проверка дисковых операций:
- Нагрузка на диск: используйте `iostat` или `iotop`.

## Проверка кэширования.

## Проверка конфигурации Nginx.

# Task 7: Ответ на 7 вопрос

## Потеря данных:
- Если система обработки задач (например, Celery) или брокер сообщений (например, RabbitMQ, Redis) выходит из строя, задачи могут быть потеряны.

## Дублирование задач:
- В некоторых случаях задача может быть отправлена или обработана несколько раз.

## Непоследовательность данных:
- Асинхронные задачи могут выполняться в произвольном порядке, что может привести к непоследовательности данных.

## Проблемы с транзакциями:
- Если задача выполняется вне транзакции базы данных, это может привести к несогласованности данных.

## Ошибки сериализации/десериализации.

## Проблемы с масштабированием.

## Проблемы с временем выполнения.

# Task 8

1. Перейти в папку `Task 8`.
2. Открыть файл `config.py` и настроить ваш SMTP: вставьте ваш email и пароль (это нужно для того, чтобы отправлять письма пользователям).
3. Собрать Docker-контейнер командой:

   ```bash
   docker-compose up --build
   ```

4. В браузере перейти по адресу:

   [http://localhost:8000/upload/](http://localhost:8000/upload/)

   Также перейти по адресу:

   [http://localhost:5555/](http://localhost:5555/) (запущен Flower, где вы можете отслеживать операции).

5. В поле email впишите email, куда придет оповещение о загрузке файла.

6. В поле csv вставьте любой CSV-файл. Через 60 секунд вы получите оповещение об успешной загрузке, и на email придет уведомление.
```